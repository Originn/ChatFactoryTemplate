//makechain.ts

import { OpenAI } from 'langchain/llms/openai';
import { PineconeStore } from 'langchain/vectorstores/pinecone';
import { ConversationalRetrievalQAChain } from 'langchain/chains';
import { CallbackManager } from "langchain/callbacks";
import { MyDocument } from 'utils/GCSLoader';
import { BufferMemory } from "langchain/memory";
import { PromptTemplate } from "langchain/prompts";
import { waitForUserInput } from './textsplitter';


// type ChatEntry = {
//   question: string;
//   answer: string;
//   score: number;
// };


const roomMemories: Record<string, BufferMemory> = {};
//const roomChatHistories: Record<string, ChatEntry[]> = {};

const MODEL_NAME = process.env.MODEL_NAME;

//CONDENSE_PROMPT is reliable for generating new question which is ied with the chat history 
const CONDENSE_PROMPT = `Given the history of the conversation and a follow up question, rephrase the follow up question to be a standalone question.
If the follow up question does not need context, return the exact same text back.
Never rephrase the follow up question given the chat history unless the follow up question needs context.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:`;

// QA_PROMPT is reliable for specific request from us, and also incoporating the new question generated by the CONDENSE_PROMPT
const QA_PROMPT = `You are a helpful AI assistant trained to provide guidance on SolidCAM in the user language from your general knowledge and context.
In instances where the question diverges from the SolidCAM context, indicate that you're optimized 
to address queries exclusively related to SolidCAM.
Don't answer questions about iMachining only if SPECIFICALLY REQUESTED! If a solution or answer is beyond your knowledge scope, 
simply admit you don't know. Avoid creating fabricated answers.

Answer in a concise or elaborate format as per the intent of the question. Use formating ** to bold, __ to italic & ~~ to cut wherever required. Format the answer using headings, paragraphs or points wherever applicable. 
=========
{context}
=========
Question: {question}
Answer in the {language} language :`;

const TRANSLATION_PROMPT = `Translate the following text to English:\nText: "{question}"`;

// Function to translate text to English using OpenAI
async function translateToEnglish(question: string, nonStreamingModel: OpenAI): Promise<string> {
  const response = await nonStreamingModel.generate([TRANSLATION_PROMPT.replace('{question}', question)]);

  // Extract the translated text from the response
  if (response.generations.length > 0 && response.generations[0].length > 0) {
    const firstGeneration = response.generations[0][0];
    const translatedText = firstGeneration.text.trim();
    console.log('translatedText:', translatedText);
    //await waitForUserInput();
    return translatedText;
  }

  // Return an empty string or a default message if no translation is found
  return 'Translation not available.';
}

const LANGUAGE_DETECTION_PROMPT = `Detect the language of the following text and respond with the language name only, nothing else:\n\nText: "{text}"`;

async function detectLanguageWithOpenAI(text: string, nonStreamingModel: OpenAI): Promise<string> {
  const prompt = LANGUAGE_DETECTION_PROMPT.replace('{text}', text);
  const response = await nonStreamingModel.generate([prompt]);

  if (response.generations.length > 0 && response.generations[0].length > 0) {
    const firstGeneration = response.generations[0][0];
    return firstGeneration.text.trim();
  }

  return 'unknown'; // or any default language code
}
// A function to detect language using `franc`
// function detectLanguage(text: string): string {
//   // Use the 'detect' method of the lngDetector instance
//   //const detectedLanguages = lngDetector.detect(text, 1); // Detects the top 1 language

//   if (detectedLanguages.length > 0) {
//     const [language, confidence] = detectedLanguages[0];
//     // You might want to check the confidence level here
//     return language;
//   }

//   // Return 'en' or any other default language if detection is not successful
//   return 'en';
// }

const TEMPRATURE = parseFloat(process.env.TEMPRATURE || "0");

export const makeChain = (vectorstore: PineconeStore, onTokenStream: (token: string) => void) => {
  const streamingModel = new OpenAI({
    streaming: true,
    modelName: MODEL_NAME,
    temperature: TEMPRATURE,
    callbacks: [
      {
        handleLLMNewToken: (token) => {
          onTokenStream(token); // Forward the streamed token to the front-end
        },
      },
    ],
  });

  // Non-streaming model setup
  const nonStreamingModel = new OpenAI({
    modelName: 'gpt-4',
    temperature: TEMPRATURE
  });

  return {
    call: async (question: string, Documents: MyDocument[], roomId: string) => {
      if (!roomMemories[roomId]) {
        roomMemories[roomId] = new BufferMemory({
          memoryKey: "chat_history",
          inputKey: "question",
          outputKey: "text",
        });
      }

      // if (!roomChatHistories[roomId]) {
      //   roomChatHistories[roomId] = [];
      // }
      let chat_history = roomMemories[roomId];
      console.log('question:', question);
      const language = await detectLanguageWithOpenAI(question, nonStreamingModel);
      console.log('language:', language);

      if (language !== 'English') {
        question = await translateToEnglish(question, nonStreamingModel);
      }

      const formattedPrompt = QA_PROMPT
        //.replace('{question}', question)
        .replace('{language}', language);

      // Use the specific room memory for the chain
      const chain = ConversationalRetrievalQAChain.fromLLM(
        streamingModel,
        vectorstore.asRetriever(6),
        {
          memory: chat_history,
          questionGeneratorChainOptions: {
            llm: nonStreamingModel,
          },
          qaTemplate: formattedPrompt,
          questionGeneratorTemplate: CONDENSE_PROMPT,
          returnSourceDocuments: true,
          verbose: true
        }
      );

      

      const responseText = (await (async () => {
        const response = await chain.call({
            question: question,
        });
        return response.text;
      })());

      Documents.unshift({ responseText } as any);

      let totalScore = 0;
      let count = 0;
      if (Documents && Documents.length > 0) {
        for (let doc of Documents) {
          if (doc.metadata) {
            totalScore += doc.metadata.score || 0;
            count++;
          }
        }
        totalScore = count > 0 ? totalScore / count : 0;
      }

      console.log('totalScore', totalScore);
      console.log('Documents', Documents);
      //await waitForUserInput();
  

        // Update the chat history with the new question, answer, and average score
        // chatHistory.push({
        //   question: question,
        //   answer: (Documents[0] as any).responseText,
        //   score: totalScore,
        // });
        
        // Filter the chat history by score
        const SCORE_THRESHOLD = 0.02;
        //chatHistory = chatHistory.filter(entry => entry.score >= SCORE_THRESHOLD);
  
        // Manage chat history size
        const MAX_HISTORY_LENGTH = 10;
        // if (chatHistory.length > MAX_HISTORY_LENGTH) {
        //   chatHistory = chatHistory.slice(-MAX_HISTORY_LENGTH);
        // }
  
        // Update roomChatHistories with the filtered and truncated chatHistory
        //roomChatHistories[roomId] = chatHistory;
  
        return Documents;
      },
      vectorstore,
    };
};

