//makechain.ts

import { OpenAI } from 'langchain/llms/openai';
import { PineconeStore } from 'langchain/vectorstores/pinecone';
import { ConversationalRetrievalQAChain } from 'langchain/chains';
//import { CallbackManager } from "langchain/callbacks";
import { MyDocument } from 'utils/GCSLoader';
import { BufferMemory } from "langchain/memory";
//import { PromptTemplate } from "langchain/prompts";
import { BaseRetriever } from "langchain/schema/retriever";
//import { waitForUserInput } from './textsplitter';
import { getIO } from "@/socketServer.cjs";
import { v4 as uuidv4 } from 'uuid';
import { insertQA } from '../db';

// Type Definitions
type SearchResult = [MyDocument, number];

// Utility Functions
async function detectLanguageWithOpenAI(text: string, nonStreamingModel: OpenAI): Promise<string> {
  const prompt = LANGUAGE_DETECTION_PROMPT.replace('{text}', text);
  const response = await nonStreamingModel.generate([prompt]);

  if (response.generations.length > 0 && response.generations[0].length > 0) {
    const firstGeneration = response.generations[0][0];
    return firstGeneration.text.trim();
  }

  return 'English';
}

async function filteredSimilaritySearch(vectorStore: any, queryText: string, type: string, limit: number, minScore: number): Promise<SearchResult[]> {
  try {
    const results: SearchResult[] = await vectorStore.similaritySearchWithScore(queryText, limit, { type: type });

    // Explicitly type the destructured elements in the filter method
    const filteredResults = results.filter(([document, score]: SearchResult) => score >= minScore);

    return filteredResults;
  } catch (error) {
    console.error("Error in filteredSimilaritySearch:", error);
    return [];
  }
}

async function translateToEnglish(question: string, nonStreamingModel: OpenAI): Promise<string> {
  const response = await nonStreamingModel.generate([TRANSLATION_PROMPT.replace('{question}', question)]);

  // Extract the translated text from the response
  if (response.generations.length > 0 && response.generations[0].length > 0) {
    const firstGeneration = response.generations[0][0];
    const translatedText = firstGeneration.text.trim();
    return translatedText;
  }

  // Return an empty string or a default message if no translation is found
  return 'Translation not available.';
}

// Class Definitions
class CustomRetriever extends BaseRetriever {
  lc_namespace = [];

  constructor(private vectorStore: PineconeStore) {
    super();
  }

  async getRelevantDocuments(query: string): Promise<MyDocument<Record<string, any>>[]> {
    const results = await this.vectorStore.similaritySearchWithScore(query, 6);
    // Map each result to include the document and its score inside metadata
    return results.map(([doc, score]) => {
      // Create a new 'metadata' object, preserving existing properties
      const newMetadata = {
        ...doc.metadata,
        score: score // Set the score property inside metadata
      };

      return new MyDocument({
        ...doc,
        metadata: newMetadata
      });
    });
  }
  async storeEmbeddings(query: string, minScoreSourcesThreshold: number) {
    const pdfResults = await filteredSimilaritySearch(
      this.vectorStore, query, 'pdf', 2, minScoreSourcesThreshold
    );
    const webinarResults = await filteredSimilaritySearch(
      this.vectorStore, query, 'youtube', 2, minScoreSourcesThreshold
    );
    const sentinelResults = await filteredSimilaritySearch(
      this.vectorStore, query, 'sentinel', 2, minScoreSourcesThreshold
    );

    const combinedResults = [...pdfResults, ...webinarResults,...sentinelResults];

    combinedResults.sort((a, b) => b[1] - a[1]);

    return combinedResults;
  }
  // Implement any other abstract methods or properties required by BaseRetriever
}

// Constants and Variables
const io = getIO();
const roomMemories: Record<string, BufferMemory> = {};
const MODEL_NAME = process.env.MODEL_NAME;
const CONDENSE_PROMPT = `Given the history of the conversation and a follow up question, rephrase the follow up question to be a standalone question.
If the follow up question does not need context, return the exact same text back.
Never rephrase the follow up question given the chat history unless the follow up question needs context.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:`;

// QA_PROMPT is reliable for specific request from us, and also incoporating the new question generated by the CONDENSE_PROMPT
const QA_PROMPT = `You are a helpful AI assistant trained to provide guidance on SolidCAM in the user language from your general knowledge and context.
In instances where the question diverges from the SolidCAM context, indicate that you're optimized 
to address queries exclusively related to SolidCAM.
Don't answer questions about iMachining only if SPECIFICALLY REQUESTED! If a solution or answer is beyond your knowledge scope, 
simply admit you don't know. Avoid creating fabricated answers.

Answer in a concise or elaborate format as per the intent of the question. Use formating ** to bold, __ to italic & ~~ to cut wherever required. Format the answer using headings, paragraphs or points wherever applicable. 
=========
{context}
=========
Question: {question}
Answer in the {language} language :`;

const TRANSLATION_PROMPT = `Translate the following text to English:\nText: "{question}"`;
const LANGUAGE_DETECTION_PROMPT = `Detect the language of the following text and respond with the language name only, nothing else:\n\nText: "{text}"`;
const TEMPRATURE = parseFloat(process.env.TEMPRATURE || "0");

export const makeChain = (vectorstore: PineconeStore, onTokenStream: (token: string) => void) => {
  const streamingModel = new OpenAI({
    streaming: true,
    modelName: MODEL_NAME,
    temperature: TEMPRATURE,
    callbacks: [
      {
        handleLLMNewToken: (token) => {
          onTokenStream(token); // Forward the streamed token to the front-end
        },
      },
    ],
  });

  // Non-streaming model setup
  const nonStreamingModel = new OpenAI({
    modelName: 'gpt-4',
    temperature: TEMPRATURE
  });

  function generateUniqueId(): string {
    return uuidv4();
  }
  return {
    call: async (question: string, Documents: MyDocument[], roomId: string, session: any) => {
      const userEmail = session?.user?.email || 'unknown';
      const qaId = generateUniqueId();

      if (!roomMemories[roomId]) {
        roomMemories[roomId] = new BufferMemory({
          memoryKey: "chat_history",
          inputKey: "question",
          outputKey: "text",
        });
      }
      let chat_history = roomMemories[roomId];
      const language = await detectLanguageWithOpenAI(question, nonStreamingModel);

      if (language !== 'English') {
        question = await translateToEnglish(question, nonStreamingModel);
      }

      const formattedPrompt = QA_PROMPT
        .replace('{language}', language);

      const customRetriever = new CustomRetriever(vectorstore);
      // Use the specific room memory for the chain
      const chain = ConversationalRetrievalQAChain.fromLLM(
        streamingModel,
        customRetriever,
        {
          memory: chat_history,
          questionGeneratorChainOptions: {
            llm: nonStreamingModel,
          },
          qaTemplate: formattedPrompt,
          questionGeneratorTemplate: CONDENSE_PROMPT,
          returnSourceDocuments: true,
          verbose: true
        }
      );

      const responseText = (await (async () => {
        const response = await chain.call({
            question: question,
        });
        return {
          text: response.text,
          sourceDocuments: response.sourceDocuments
        };
      })());
      const minScoreSourcesThreshold = process.env.MINSCORESOURCESTHRESHOLD !== undefined ? parseFloat(process.env.MINSCORESOURCESTHRESHOLD) : 0.7;
      let embeddingsStore = await customRetriever.storeEmbeddings(responseText.text, minScoreSourcesThreshold);

      for (const [doc, score] of embeddingsStore) {
        const myDoc = new MyDocument({
          pageContent: doc.pageContent,
          metadata: {
            source: doc.metadata.source,     
            type: doc.metadata.type,         
            videoLink: doc.metadata.videoLink,
            score: score                     
          }
        });
      
        Documents.push(myDoc);
      }

      if (roomId) {
        console.log("INSIDE ROOM_ID", roomId);     
        io.to(roomId).emit(`fullResponse-${roomId}`, {
          roomId: roomId,
          sourceDocs: Documents,
          qaId: qaId
        });
      } else {
        io.emit("fullResponse", {
          sourceDocs: Documents,
          qaId: qaId
        });
      }

      await insertQA(question, responseText.text, responseText.sourceDocuments, Documents, qaId, roomId, userEmail);
    
      let totalScore = 0;
      let count = 0;
      if (Documents && Documents.length > 0) {
        for (let doc of Documents) {
          if (doc.metadata) {
            totalScore += doc.metadata.score || 0;
            count++;
          }
        }
        totalScore = count > 0 ? totalScore / count : 0;
      }

      //await waitForUserInput();
  

        // Update the chat history with the new question, answer, and average score
        // chatHistory.push({
        //   question: question,
        //   answer: (Documents[0] as any).responseText,
        //   score: totalScore,
        // });
        
        // Filter the chat history by score
        const SCORE_THRESHOLD = 0.02;
        //chatHistory = chatHistory.filter(entry => entry.score >= SCORE_THRESHOLD);
  
        // Manage chat history size
        const MAX_HISTORY_LENGTH = 10;
        // if (chatHistory.length > MAX_HISTORY_LENGTH) {
        //   chatHistory = chatHistory.slice(-MAX_HISTORY_LENGTH);
        // }
  
        // Update roomChatHistories with the filtered and truncated chatHistory
        //roomChatHistories[roomId] = chatHistory;
  
        return Documents;
      },
      vectorstore,
    };
};

